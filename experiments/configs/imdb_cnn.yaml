project:
  seed: 1337
  device: "cpu"        # "cuda", "cpu", or "auto"

data:
  root: "data/raw/aclImdb"
  use_official_test: true
  val_ratio: 0.2
  lowercase: true
  remove_html_breaks: true
  normalize_unicode: true
  max_len: 400      # was 256
  min_freq: 1       # was 3
  max_vocab: 20000
  pad_token: "<pad>"
  unk_token: "<unk>"
  num_workers: 4
  batch_size: 64

model:
  name: "cnn_text"
  embedding_dim: 200      # was 100
  filter_sizes: [3, 4, 5]
  num_filters: 150        # was 100
  dropout: 0.5
  num_classes: 2

train:
  epochs: 20
  optimizer: "adam"
  lr: 0.001
  weight_decay: 0.0
  early_stop_patience: 5
  grad_clip: 5.0
  mixed_precision: false

log:
  dir: "experiments/logs"
  csv_name: "imdb_cnn_clean.csv"
  ckpt_path: "experiments/logs/imdb_cnn.pt"

attacks:
  # evaluate on this many test examples (None = full test set)
  max_eval_examples: 500    # you can raise/lower this later

  keyword:
    max_changes_list: [1, 2, 3]   # budgets k for word substitutions
    max_fraction_changed: 0.2     # ≤20% of real tokens can be changed
    top_k_words: 8                # only attack top-8 important words
    max_synonyms: 20              # nearest neighbours to consider per word

  char:
    max_edits_list: [1, 2, 3]     # budgets m for character edits
    max_char_frac: 0.15           # ≤15% of characters may be modified

adv_train:
  from_ckpt: "experiments/logs/imdb_cnn.pt"        # start from clean baseline
  out_ckpt: "experiments/logs/imdb_cnn_adv.pt"     # save fine-tuned model here
  log_csv:  "experiments/logs/imdb_cnn_adv.csv"    # training log for adv training

  epochs: 3              # 3–5 is usually enough as fine-tuning
  lr: 0.0005             # smaller LR for fine-tuning than baseline (0.001)
  weight_decay: 0.0

  adv_ratio: 0.5         # fraction of each batch to turn into adversarial
  attacks: ["keyword", "char"]   # which attacks to sample from
  max_changes: [1, 2]    # budgets for generating adversarial samples during training
